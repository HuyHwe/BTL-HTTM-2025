{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72174e76",
   "metadata": {},
   "source": [
    "## Bước 1: Tạo X, y là bộ dữ liệu từ ảnh png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a0bcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2712, 144, 192, 3) (2712,)\n"
     ]
    }
   ],
   "source": [
    "# Merge all the code into a single script\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def parse_png_to_tensor(image_path: str) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Parses a PNG image file into a TensorFlow tensor.\n",
    "\n",
    "    This function is updated to be compatible with a TensorFlow training loop.\n",
    "    It converts the image to RGB format, normalizes the pixel values to a\n",
    "    0-1 range, and adds a batch dimension.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The file path to the PNG image.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: A TensorFlow tensor of the image with a shape of\n",
    "                   (1, height, width, 3). Returns an empty tensor if the file\n",
    "                   cannot be processed.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: File not found at {image_path}\")\n",
    "        return tf.empty(0)\n",
    "\n",
    "    try:\n",
    "        # Open the image file\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Convert the image to RGB format. This handles different modes like RGBA or P.\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        # Convert the PIL Image object to a NumPy array.\n",
    "        # The shape of the array will be (height, width, channels)\n",
    "        numpy_array = np.array(image, dtype=np.float32)\n",
    "        \n",
    "        # Normalize the pixel values from 0-255 to 0.0-1.0\n",
    "        normalized_array = numpy_array / 255.0\n",
    "        \n",
    "        # Convert the NumPy array to a TensorFlow tensor.\n",
    "        # The tensor will have a shape of (height, width, channels)\n",
    "        image_tensor = tf.convert_to_tensor(normalized_array)\n",
    "        \n",
    "        # Add a batch dimension at the beginning to match the expected\n",
    "        # input shape for a model (batch_size, height, width, channels)\n",
    "        final_tensor = tf.expand_dims(image_tensor, axis=0)\n",
    "\n",
    "        return final_tensor\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the image: {e}\")\n",
    "        return tf.empty(0)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create a dummy PNG file for testing\n",
    "#     dummy_image_path = \"test_image.png\"\n",
    "    \n",
    "#     # Create a 100x100 white image with RGB channels\n",
    "#     dummy_image = Image.new('RGB', (100, 100), 'white')\n",
    "#     dummy_image.save(dummy_image_path)\n",
    "#     print(f\"Created a dummy image at: {dummy_image_path}\")\n",
    "\n",
    "#     # Call the parse function on the dummy image\n",
    "#     image_tensor = parse_png_to_tensor(dummy_image_path)\n",
    "\n",
    "#     # Check the result\n",
    "#     if image_tensor.shape.num_elements() > 0:\n",
    "#         print(\"\\nSuccessfully converted image to TensorFlow tensor!\")\n",
    "#         print(f\"Tensor shape: {image_tensor.shape}\")\n",
    "#         print(f\"Data type: {image_tensor.dtype}\")\n",
    "#     else:\n",
    "#         print(\"\\nFailed to convert image to tensor.\")\n",
    "\n",
    "#     # Clean up the dummy file\n",
    "#     os.remove(dummy_image_path)\n",
    "#     print(f\"\\nCleaned up dummy file: {dummy_image_path}\")\n",
    "\n",
    "\n",
    "IMG_SIZE = [144, 192]\n",
    "labels = [l for l in os.listdir(\"./dataset\") if l != \".DS_Store\"]\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "for i, label in enumerate(labels):\n",
    "    img_list = os.listdir(f\"./dataset/{label}/\")\n",
    "    for img in img_list:\n",
    "        img_path = f\"./dataset/{label}/{img}\"\n",
    "        tensor = parse_png_to_tensor(img_path)\n",
    "        # Resize and check for valid tensor\n",
    "        if tensor.shape.num_elements() > 0:\n",
    "            resized = tf.image.resize(tf.squeeze(tensor, axis=0), IMG_SIZE)\n",
    "            X_list.append(resized)\n",
    "            y_list.append(i)\n",
    "\n",
    "X = tf.stack(X_list, axis=0)\n",
    "y = tf.convert_to_tensor(y_list, dtype=tf.int32)\n",
    "print(X.shape, y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d7034a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2712, 144, 192, 3]), TensorShape([2712]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e13c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE = [144,192]\n",
    "NUM_CLASSES = 12\n",
    "BATCH = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e32838d",
   "metadata": {},
   "source": [
    "## CNN without Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85c53a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_init(shape):\n",
    "    fan_in = tf.cast(tf.math.reduce_prod(shape[:-1]), tf.float32)\n",
    "    std = tf.sqrt(2.0/fan_in)\n",
    "    return tf.random.normal(shape, stddev=std)\n",
    "\n",
    "class SimpleCNN:\n",
    "    def __init__(self, num_classes=12):\n",
    "        self.W1 = tf.Variable(he_init([3,3,3,32]));  self.b1 = tf.Variable(tf.zeros([32]))\n",
    "        self.W2 = tf.Variable(he_init([3,3,32,64])); self.b2 = tf.Variable(tf.zeros([64]))\n",
    "        self.W3 = tf.Variable(he_init([3,3,64,128]));self.b3 = tf.Variable(tf.zeros([128]))\n",
    "        \n",
    "        # We will initialize the weights for the dense layers dynamically\n",
    "        # based on the input image shape.\n",
    "        self.W4 = None\n",
    "        self.b4 = None\n",
    "        self.W5 = tf.Variable(he_init([256, num_classes]))\n",
    "        self.b5 = tf.Variable(tf.zeros([num_classes]))\n",
    "        self.weights_initialized = False\n",
    "\n",
    "    def __call__(self, x, training=False, drop_rate=0.3):\n",
    "        # x: [B, H, W, 3]\n",
    "        x = tf.nn.conv2d(x, self.W1, strides=1, padding='SAME') + self.b1\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
    "\n",
    "        x = tf.nn.conv2d(x, self.W2, strides=1, padding='SAME') + self.b2\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
    "\n",
    "        x = tf.nn.conv2d(x, self.W3, strides=1, padding='SAME') + self.b3\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
    "\n",
    "        if not self.weights_initialized:\n",
    "            # Dynamically calculate the flattened dimension and initialize weights\n",
    "            # This is done the first time the model is called with a new input shape\n",
    "            flat_dim = tf.math.reduce_prod(x.shape[1:])\n",
    "            self.W4 = tf.Variable(he_init([flat_dim, 256]))\n",
    "            self.b4 = tf.Variable(tf.zeros([256]))\n",
    "            self.weights_initialized = True\n",
    "\n",
    "        x = tf.reshape(x, [-1, tf.math.reduce_prod(x.shape[1:])])\n",
    "        x = tf.matmul(x, self.W4) + self.b4\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        if training and drop_rate > 0:\n",
    "            keep = 1.0 - drop_rate\n",
    "            mask = tf.cast(tf.random.uniform(tf.shape(x)) < keep, x.dtype)\n",
    "            x = (x * mask) / keep\n",
    "\n",
    "        logits = tf.matmul(x, self.W5) + self.b5\n",
    "        return logits\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        # Make sure to return all variables, including the dynamically created ones\n",
    "        return [self.W1,self.b1,self.W2,self.b2,self.W3,self.b3,self.W4,self.b4,self.W5,self.b5]\n",
    "\n",
    "import math\n",
    "\n",
    "def cross_entropy_loss(logits, onehot_labels):\n",
    "    per_ex = tf.nn.softmax_cross_entropy_with_logits(labels=onehot_labels, logits=logits)\n",
    "    return tf.reduce_mean(per_ex)\n",
    "\n",
    "def accuracy(logits, onehot_labels):\n",
    "    pred = tf.argmax(logits, axis=1)\n",
    "    true = tf.argmax(onehot_labels, axis=1)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(pred, true), tf.float32))\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=1e-3, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = {}\n",
    "\n",
    "    def apply_gradients(self, grads, vars):\n",
    "        for g, v in zip(grads, vars):\n",
    "            if g is None:\n",
    "                continue\n",
    "            key = id(v)\n",
    "            if key not in self.v:\n",
    "                self.v[key] = tf.zeros_like(v)\n",
    "            self.v[key] = self.momentum*self.v[key] + g\n",
    "            v.assign_sub(self.lr * self.v[key])\n",
    "\n",
    "def train(model, ds_train, ds_val, epochs=15, lr=1e-3):\n",
    "    opt = SGD(lr=lr, momentum=0.9)\n",
    "    for ep in range(1, epochs+1):\n",
    "        tr_loss = tf.metrics.Mean(); tr_acc = tf.metrics.Mean()\n",
    "        for x,y in ds_train:\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x, training=True)\n",
    "                loss = cross_entropy_loss(logits, y)\n",
    "            grads = tape.gradient(loss, model.variables)\n",
    "            opt.apply_gradients(grads, model.variables)\n",
    "\n",
    "            tr_loss.update_state(loss)\n",
    "            tr_acc.update_state(accuracy(logits, y))\n",
    "\n",
    "        va_loss = tf.metrics.Mean(); va_acc = tf.metrics.Mean()\n",
    "        for x,y in ds_val:\n",
    "            logits = model(x, training=False)\n",
    "            va_loss.update_state(cross_entropy_loss(logits, y))\n",
    "            va_acc.update_state(accuracy(logits, y))\n",
    "\n",
    "        print(f\"Epoch {ep:02d} | train_loss={tr_loss.result():.4f} acc={tr_acc.result():.4f} \"\n",
    "              f\"| val_loss={va_loss.result():.4f} acc={va_acc.result():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b8263ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating pre-existing X and y tensors...\n",
      "Loaded X shape: (2712, 144, 192, 3)\n",
      "Loaded y shape: (2712,)\n",
      "\n",
      "Starting CNN training...\n",
      "Epoch 01 | train_loss=2.3696 acc=0.1949 | val_loss=1.9582 acc=0.3999\n",
      "Epoch 02 | train_loss=1.6791 acc=0.4260 | val_loss=1.2268 acc=0.6279\n",
      "Epoch 03 | train_loss=1.0446 acc=0.6425 | val_loss=0.8838 acc=0.7027\n",
      "Epoch 04 | train_loss=0.7108 acc=0.7583 | val_loss=0.5284 acc=0.8564\n",
      "Epoch 05 | train_loss=0.4624 acc=0.8456 | val_loss=0.3005 acc=0.9152\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Simulate pre-existing X and y tensors as requested ---\n",
    "    NUM_CLASSES = 12\n",
    "    NUM_IMAGES = 2712\n",
    "    IMAGE_HEIGHT = 144\n",
    "    IMAGE_WIDTH = 192\n",
    "\n",
    "    print(\"\\nSimulating pre-existing X and y tensors...\")\n",
    "\n",
    "\n",
    "    print(f\"Loaded X shape: {X.shape}\")\n",
    "    print(f\"Loaded y shape: {y.shape}\")\n",
    "\n",
    "    # --- Prepare the data for training ---\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_one_hot = tf.one_hot(y, NUM_CLASSES)\n",
    "    \n",
    "    # Shuffle and split the dataset\n",
    "    dataset_size = len(y)\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "    \n",
    "    full_dataset = tf.data.Dataset.from_tensor_slices((X, y_one_hot))\n",
    "    full_dataset = full_dataset.shuffle(buffer_size=dataset_size)\n",
    "    \n",
    "    ds_train = full_dataset.take(train_size).batch(8)\n",
    "    ds_val = full_dataset.skip(train_size).batch(8)\n",
    "    \n",
    "    # --- Train the model ---\n",
    "    print(\"\\nStarting CNN training...\")\n",
    "    model = SimpleCNN(num_classes=NUM_CLASSES)\n",
    "    train(model, ds_train, ds_val, epochs=5, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2213f80",
   "metadata": {},
   "source": [
    "### Test frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0107dce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 11, 0, 0, 0, 0, 0, 0]\n",
      "[0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "test = X[:8]\n",
    "res = [tf.argmax(model(test)[i]).numpy() for i in range(8)]\n",
    "print(res)\n",
    "# print((model(X[:8], training=False)))\n",
    "print(y[:8].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d091099d",
   "metadata": {},
   "source": [
    "## CNN with Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bb63a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 07:34:50.950703: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - ETA: 0s - loss: 1.1773 - accuracy: 0.6266\n",
      "Epoch 1: val_accuracy improved from -inf to 0.57459, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 12s 40ms/step - loss: 1.1773 - accuracy: 0.6266 - val_loss: 1.2776 - val_accuracy: 0.5746\n",
      "Epoch 2/20\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.4301 - accuracy: 0.8645\n",
      "Epoch 2: val_accuracy improved from 0.57459 to 0.61326, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 10s 37ms/step - loss: 0.4301 - accuracy: 0.8645 - val_loss: 1.1798 - val_accuracy: 0.6133\n",
      "Epoch 3/20\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.2745 - accuracy: 0.9175\n",
      "Epoch 3: val_accuracy improved from 0.61326 to 0.76243, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 10s 36ms/step - loss: 0.2745 - accuracy: 0.9175 - val_loss: 0.7536 - val_accuracy: 0.7624\n",
      "Epoch 4/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.9571\n",
      "Epoch 4: val_accuracy improved from 0.76243 to 0.76611, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 10s 37ms/step - loss: 0.1558 - accuracy: 0.9567 - val_loss: 0.7078 - val_accuracy: 0.7661\n",
      "Epoch 5/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9668\n",
      "Epoch 5: val_accuracy improved from 0.76611 to 0.78821, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 10s 36ms/step - loss: 0.1269 - accuracy: 0.9663 - val_loss: 0.6732 - val_accuracy: 0.7882\n",
      "Epoch 6/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.1111 - accuracy: 0.9659\n",
      "Epoch 6: val_accuracy improved from 0.78821 to 0.79742, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 10s 37ms/step - loss: 0.1121 - accuracy: 0.9659 - val_loss: 0.6224 - val_accuracy: 0.7974\n",
      "Epoch 7/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9834\n",
      "Epoch 7: val_accuracy improved from 0.79742 to 0.92449, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 10s 38ms/step - loss: 0.0752 - accuracy: 0.9829 - val_loss: 0.2334 - val_accuracy: 0.9245\n",
      "Epoch 8/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9719\n",
      "Epoch 8: val_accuracy did not improve from 0.92449\n",
      "272/272 [==============================] - 11s 39ms/step - loss: 0.0967 - accuracy: 0.9714 - val_loss: 0.5415 - val_accuracy: 0.8177\n",
      "Epoch 9/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9811\n",
      "Epoch 9: val_accuracy did not improve from 0.92449\n",
      "272/272 [==============================] - 11s 41ms/step - loss: 0.0688 - accuracy: 0.9806 - val_loss: 0.2557 - val_accuracy: 0.9079\n",
      "Epoch 10/20\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9829\n",
      "Epoch 10: val_accuracy did not improve from 0.92449\n",
      "272/272 [==============================] - 11s 41ms/step - loss: 0.0665 - accuracy: 0.9829 - val_loss: 0.4816 - val_accuracy: 0.8913\n",
      "Epoch 11/20\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9811\n",
      "Epoch 11: val_accuracy did not improve from 0.92449\n",
      "272/272 [==============================] - 12s 43ms/step - loss: 0.0614 - accuracy: 0.9811 - val_loss: 0.9023 - val_accuracy: 0.7422\n",
      "Epoch 12/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.0565 - accuracy: 0.9834\n",
      "Epoch 12: val_accuracy improved from 0.92449 to 0.93370, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 13s 48ms/step - loss: 0.0576 - accuracy: 0.9829 - val_loss: 0.1940 - val_accuracy: 0.9337\n",
      "Epoch 13/20\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.9848\n",
      "Epoch 13: val_accuracy did not improve from 0.93370\n",
      "272/272 [==============================] - 13s 48ms/step - loss: 0.0569 - accuracy: 0.9848 - val_loss: 0.4247 - val_accuracy: 0.8656\n",
      "Epoch 14/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9797\n",
      "Epoch 14: val_accuracy did not improve from 0.93370\n",
      "272/272 [==============================] - 13s 47ms/step - loss: 0.0706 - accuracy: 0.9793 - val_loss: 0.5253 - val_accuracy: 0.8343\n",
      "Epoch 15/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.0453 - accuracy: 0.9880\n",
      "Epoch 15: val_accuracy did not improve from 0.93370\n",
      "272/272 [==============================] - 13s 48ms/step - loss: 0.0463 - accuracy: 0.9880 - val_loss: 0.1782 - val_accuracy: 0.9282\n",
      "Epoch 16/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.0202 - accuracy: 0.9958\n",
      "Epoch 16: val_accuracy did not improve from 0.93370\n",
      "272/272 [==============================] - 15s 57ms/step - loss: 0.0216 - accuracy: 0.9954 - val_loss: 0.5685 - val_accuracy: 0.8122\n",
      "Epoch 17/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.0395 - accuracy: 0.9885\n",
      "Epoch 17: val_accuracy did not improve from 0.93370\n",
      "272/272 [==============================] - 19s 70ms/step - loss: 0.0406 - accuracy: 0.9880 - val_loss: 1.5761 - val_accuracy: 0.7072\n",
      "Epoch 18/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.0458 - accuracy: 0.9843\n",
      "Epoch 18: val_accuracy did not improve from 0.93370\n",
      "272/272 [==============================] - 18s 67ms/step - loss: 0.0468 - accuracy: 0.9839 - val_loss: 0.4066 - val_accuracy: 0.8803\n",
      "Epoch 19/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.0550 - accuracy: 0.9792\n",
      "Epoch 19: val_accuracy did not improve from 0.93370\n",
      "272/272 [==============================] - 17s 64ms/step - loss: 0.0557 - accuracy: 0.9793 - val_loss: 0.4164 - val_accuracy: 0.9079\n",
      "Epoch 20/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9760\n",
      "Epoch 20: val_accuracy did not improve from 0.93370\n",
      "272/272 [==============================] - 16s 60ms/step - loss: 0.0757 - accuracy: 0.9756 - val_loss: 0.4159 - val_accuracy: 0.8692\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Utils function\n",
    "# Save best model check point\n",
    "checkpoint_cb = ModelCheckpoint(\"cnn_best.h5\", \n",
    "                                save_best_only=True, \n",
    "                                monitor=\"val_accuracy\",    \n",
    "                                mode=\"max\",                \n",
    "                                save_weights_only=True,     \n",
    "                                verbose=1)\n",
    "\n",
    "# He initializer function\n",
    "def he_init(shape, dtype=None):\n",
    "    fan_in = tf.cast(tf.reduce_prod(shape[:-1]), tf.float32)\n",
    "    std = tf.sqrt(2.0 / fan_in)\n",
    "    return tf.random.normal(shape, stddev=std)\n",
    "\n",
    "NUM_CLASSES = 12 \n",
    "\n",
    "# Early stopping\n",
    "\n",
    "earlystop_cb = EarlyStopping(\n",
    "    patience=5, restore_best_weights=True,\n",
    "    monitor=\"val_loss\", mode=\"min\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "CNN = keras.Sequential([\n",
    "    # Block 1\n",
    "    layers.Conv2D(32, (3, 3), padding=\"same\",\n",
    "                  kernel_initializer=he_init, activation=None,\n",
    "                  input_shape=(144, 192, 3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.ReLU(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Block 2\n",
    "    layers.Conv2D(64, (3, 3), padding=\"same\",\n",
    "                  kernel_initializer=he_init, activation=None),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.ReLU(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Block 3\n",
    "    layers.Conv2D(128, (3, 3), padding=\"same\",\n",
    "                  kernel_initializer=he_init, activation=None),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.ReLU(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Dense head\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, kernel_initializer=he_init, activation=None),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.ReLU(),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    # Output\n",
    "    layers.Dense(NUM_CLASSES, activation=\"softmax\", kernel_initializer=he_init),\n",
    "])\n",
    "\n",
    "# Split train/val\n",
    "X_np = X.numpy() if isinstance(X, tf.Tensor) else X\n",
    "y_np = y.numpy() if isinstance(y, tf.Tensor) else y\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_np, y_np, test_size=0.2, random_state=42, stratify=y_np\n",
    ")\n",
    "\n",
    "# Compile (match activation choice!)\n",
    "CNN.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),  # softmax used\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = CNN.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_val, y_val),\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70d92daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 36ms/step - loss: 0.1940 - accuracy: 0.9337\n",
      "Best model -> val_acc=0.9337, val_loss=0.1940\n"
     ]
    }
   ],
   "source": [
    "# After training, load back the best weights:\n",
    "CNN.load_weights(\"cnn_best.h5\")\n",
    "\n",
    "# Now CNN is restored to the best epoch\n",
    "val_loss, val_acc = CNN.evaluate(X_val, y_val)\n",
    "print(f\"Best model -> val_acc={val_acc:.4f}, val_loss={val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76c15154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 144, 192, 3)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "5\n",
      "0\n",
      "3\n",
      "0\n",
      "[3 6 2 0 5 0 3 0]\n"
     ]
    }
   ],
   "source": [
    "test = X_val[:8]\n",
    "print(test.shape)\n",
    "# res = [tf.argmax(CNN.predict(test[i])).numpy() for i in range(8)]\n",
    "res = CNN.predict(test)\n",
    "for i in range(8):\n",
    "    print(tf.argmax(res[i]).numpy())\n",
    "# print(res)\n",
    "# print((model(X[:8], training=False)))\n",
    "print(y_val[:8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8411c0",
   "metadata": {},
   "source": [
    "## CNN With pretrained model: VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4476ae24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, ReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# --- Utils ---\n",
    "def he_init(shape, dtype=None):\n",
    "    fan_in = tf.cast(tf.reduce_prod(shape[:-1]), tf.float32)\n",
    "    std = tf.sqrt(2.0 / fan_in)\n",
    "    return tf.random.normal(shape, stddev=std)\n",
    "\n",
    "NUM_CLASSES = 12\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    \"cnn_best.h5\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_weights_only=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "earlystop_cb = EarlyStopping(\n",
    "    patience=5, restore_best_weights=True,\n",
    "    monitor=\"val_loss\", mode=\"min\",\n",
    ")\n",
    "\n",
    "# --- Base model (no top) ---\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(144, 192, 3))\n",
    "base_model.trainable = False  # freeze for transfer learning\n",
    "\n",
    "# Pick a truncation point, e.g. after block2_pool or block3_pool\n",
    "truncated_output = base_model.get_layer(\"block3_pool\").output  # or \"block3_pool\"\n",
    "truncated_base = keras.Model(inputs=base_model.input, outputs=truncated_output)\n",
    "truncated_base.trainable = False\n",
    "\n",
    "# --- Build model ---\n",
    "inputs = Input(shape=(144, 192, 3))\n",
    "\n",
    "# If your images are in [0,255]: use preprocess_input(inputs)\n",
    "# If your images are in [0,1]: multiply by 255 first\n",
    "x = preprocess_input(inputs * 255.0)\n",
    "\n",
    "x = truncated_base(x, training=False)\n",
    "x = GlobalAveragePooling2D()(x)  # <-- only once!\n",
    "\n",
    "# Small head\n",
    "x = Dense(256, kernel_initializer=he_init, activation=None)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "outputs = Dense(NUM_CLASSES, activation=\"softmax\", kernel_initializer=he_init)(x)\n",
    "\n",
    "vgg_model = keras.Model(inputs, outputs, name=\"VGG16_transfer\")\n",
    "\n",
    "# --- Compile ---\n",
    "vgg_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# --- Train (example) ---\n",
    "# history = vgg_model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=20,\n",
    "#     batch_size=32,\n",
    "#     callbacks=[checkpoint_cb, earlystop_cb],\n",
    "#     shuffle=True,\n",
    "# )\n",
    "# vgg_model.load_weights(\"cnn_best.h5\")  # ensure best weights loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba10cb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 11:14:42.778459: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - ETA: 0s - loss: 1.6165 - accuracy: 0.4901\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50829, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 24s 85ms/step - loss: 1.6165 - accuracy: 0.4901 - val_loss: 1.4618 - val_accuracy: 0.5083\n",
      "Epoch 2/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.7779 - accuracy: 0.7615\n",
      "Epoch 2: val_accuracy improved from 0.50829 to 0.58748, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 22s 81ms/step - loss: 0.7786 - accuracy: 0.7612 - val_loss: 1.1614 - val_accuracy: 0.5875\n",
      "Epoch 3/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.5378 - accuracy: 0.8399\n",
      "Epoch 3: val_accuracy did not improve from 0.58748\n",
      "272/272 [==============================] - 22s 81ms/step - loss: 0.5387 - accuracy: 0.8396 - val_loss: 1.2338 - val_accuracy: 0.5856\n",
      "Epoch 4/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.4510 - accuracy: 0.8552\n",
      "Epoch 4: val_accuracy improved from 0.58748 to 0.59484, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 22s 80ms/step - loss: 0.4518 - accuracy: 0.8552 - val_loss: 1.2097 - val_accuracy: 0.5948\n",
      "Epoch 5/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.3575 - accuracy: 0.8967\n",
      "Epoch 5: val_accuracy improved from 0.59484 to 0.81031, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 22s 82ms/step - loss: 0.3583 - accuracy: 0.8963 - val_loss: 0.5541 - val_accuracy: 0.8103\n",
      "Epoch 6/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.3312 - accuracy: 0.8902\n",
      "Epoch 6: val_accuracy improved from 0.81031 to 0.81584, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 22s 81ms/step - loss: 0.3321 - accuracy: 0.8898 - val_loss: 0.5497 - val_accuracy: 0.8158\n",
      "Epoch 7/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.9096\n",
      "Epoch 7: val_accuracy did not improve from 0.81584\n",
      "272/272 [==============================] - 22s 82ms/step - loss: 0.2970 - accuracy: 0.9092 - val_loss: 0.7650 - val_accuracy: 0.7422\n",
      "Epoch 8/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.9147\n",
      "Epoch 8: val_accuracy improved from 0.81584 to 0.83978, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 22s 80ms/step - loss: 0.2866 - accuracy: 0.9142 - val_loss: 0.4667 - val_accuracy: 0.8398\n",
      "Epoch 9/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.2553 - accuracy: 0.9188\n",
      "Epoch 9: val_accuracy improved from 0.83978 to 0.86004, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 22s 81ms/step - loss: 0.2564 - accuracy: 0.9184 - val_loss: 0.4307 - val_accuracy: 0.8600\n",
      "Epoch 10/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9207\n",
      "Epoch 10: val_accuracy did not improve from 0.86004\n",
      "272/272 [==============================] - 22s 81ms/step - loss: 0.2444 - accuracy: 0.9202 - val_loss: 0.5234 - val_accuracy: 0.8324\n",
      "Epoch 11/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.2148 - accuracy: 0.9377\n",
      "Epoch 11: val_accuracy improved from 0.86004 to 0.86924, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 22s 81ms/step - loss: 0.2157 - accuracy: 0.9373 - val_loss: 0.4307 - val_accuracy: 0.8692\n",
      "Epoch 12/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.2293 - accuracy: 0.9304\n",
      "Epoch 12: val_accuracy did not improve from 0.86924\n",
      "272/272 [==============================] - 22s 80ms/step - loss: 0.2302 - accuracy: 0.9299 - val_loss: 0.4765 - val_accuracy: 0.8564\n",
      "Epoch 13/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.1997 - accuracy: 0.9331\n",
      "Epoch 13: val_accuracy did not improve from 0.86924\n",
      "272/272 [==============================] - 22s 81ms/step - loss: 0.2005 - accuracy: 0.9331 - val_loss: 0.4166 - val_accuracy: 0.8656\n",
      "Epoch 14/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.2023 - accuracy: 0.9359\n",
      "Epoch 14: val_accuracy improved from 0.86924 to 0.88582, saving model to cnn_best.h5\n",
      "272/272 [==============================] - 22s 81ms/step - loss: 0.2032 - accuracy: 0.9355 - val_loss: 0.3466 - val_accuracy: 0.8858\n",
      "Epoch 15/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.2085 - accuracy: 0.9336\n",
      "Epoch 15: val_accuracy did not improve from 0.88582\n",
      "272/272 [==============================] - 22s 81ms/step - loss: 0.2096 - accuracy: 0.9331 - val_loss: 0.5251 - val_accuracy: 0.8361\n",
      "Epoch 16/20\n",
      "271/272 [============================>.] - ETA: 0s - loss: 0.1778 - accuracy: 0.9442\n",
      "Epoch 16: val_accuracy did not improve from 0.88582\n",
      "272/272 [==============================] - 22s 82ms/step - loss: 0.1785 - accuracy: 0.9442 - val_loss: 0.9386 - val_accuracy: 0.7238\n",
      "Epoch 17/20\n",
      "260/272 [===========================>..] - ETA: 0s - loss: 0.1808 - accuracy: 0.9389"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      6\u001b[39m y_np = y.numpy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, tf.Tensor) \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[32m      7\u001b[39m X_train, X_val, y_train, y_val = train_test_split(\n\u001b[32m      8\u001b[39m     X_np, y_np, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=y_np\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mvgg_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearlystop_cb\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodeHTTM/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodeHTTM/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[39m, in \u001b[36mModel.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[39m\n\u001b[32m   1799\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.profiler.experimental.Trace(\n\u001b[32m   1800\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1801\u001b[39m     epoch_num=epoch,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1804\u001b[39m     _r=\u001b[32m1\u001b[39m,\n\u001b[32m   1805\u001b[39m ):\n\u001b[32m   1806\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m-> \u001b[39m\u001b[32m1807\u001b[39m     tmp_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1808\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_handler.should_sync:\n\u001b[32m   1809\u001b[39m         context.async_wait()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodeHTTM/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodeHTTM/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    829\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    835\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodeHTTM/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    865\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    866\u001b[39m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[32m    867\u001b[39m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    872\u001b[39m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[32m    873\u001b[39m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[32m    874\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodeHTTM/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodeHTTM/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1319\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1321\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1322\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1323\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1324\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1325\u001b[39m     args,\n\u001b[32m   1326\u001b[39m     possible_gradient_type,\n\u001b[32m   1327\u001b[39m     executing_eagerly)\n\u001b[32m   1328\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodeHTTM/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodeHTTM/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodeHTTM/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1484\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1494\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1495\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1496\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1500\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1501\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodeHTTM/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "X_np = X.numpy() if isinstance(X, tf.Tensor) else X\n",
    "y_np = y.numpy() if isinstance(y, tf.Tensor) else y\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_np, y_np, test_size=0.2, random_state=42, stratify=y_np\n",
    ")\n",
    "\n",
    "\n",
    "vgg_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_val, y_val),\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb]\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CodeHTTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
